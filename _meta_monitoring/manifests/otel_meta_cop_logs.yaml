# OpenTelemetry Collector configuration for logs in daemonset mode

mode: "daemonset"

image:
  repository: otel/opentelemetry-collector-contrib
serviceAccount:
  name: ${service_account}
  create: false

podAnnotations:
  prometheus.io/path: /metrics
  prometheus.io/port: "8888"
  prometheus.io/scrape: "true"

presets:
  logsCollection:
    enabled: true
  hostMetrics:
    enabled: false
  kubernetesAttributes:
    enabled: true
    extractAllPodLabels: true
    extractAllPodAnnotations: true
  kubeletMetrics:
    enabled: false
  kubernetesEvents:
    enabled: true
  clusterMetrics:
    enabled: false

configMap:
  create: true

extraVolumeMounts:
  - name: varlog
    mountPath: /host/var/log
    readOnly: true

extraVolumes:
  - name: varlog
    hostPath:
      path: /var/log

config:
  receivers:
    # k8s_events receiver collects Kubernetes events
    k8s_events:
    filelog:
      include:
        - /var/log/pods/*/*/*.log
        - /host/var/log/syslog
        - /host/var/log/messages
        - /host/var/log/*.log
      # Exclude logs from specific pods (refine these patterns if needed)
      exclude:
        - /var/log/pods/*nginx*/**
        - /host/var/log/falcon-*/**
      start_at: end
      include_file_path: true
      include_file_name: true
      operators:
        # Route based on log format
        - type: router
          id: get-format
          routes:
            - output: parser-docker
              expr: 'body matches "^\\{"'
            - output: parser-crio
              expr: 'body matches "^[^ Z]+ "'
            - output: parser-containerd
              expr: 'body matches "^[^ Z]+Z"'
        # CRI-O log parser
        - type: regex_parser
          id: parser-crio
          regex: "^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$"
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout_type: gotime
            layout: "2006-01-02T15:04:05.000000000-07:00"
        # Containerd log parser
        - type: regex_parser
          id: parser-containerd
          regex: "^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$"
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout: "%Y-%m-%dT%H:%M:%S.%LZ"
        # Docker JSON log parser
        - type: json_parser
          id: parser-docker
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout: "%Y-%m-%dT%H:%M:%S.%LZ"
        # Move the log field to body for downstream processing
        - type: move
          from: attributes.log
          to: body
        # Extract metadata from the file path
        - type: regex_parser
          id: extract_metadata_from_filepath
          regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]{36})\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$'
          parse_from: attributes["log.file.path"]
        # Filter out unwanted logs (e.g. from loki canary pods)
        - type: filter
          id: junk_filter
          expr: 'attributes.container_name matches "^loki-(canary).*$"'
        # Rename attributes for clarity
        - type: move
          from: attributes.container_name
          to: attributes["container"]
        - type: move
          from: attributes.namespace
          to: attributes["namespace"]
        - type: move
          from: attributes.pod_name
          to: attributes["pod"]
        - type: move
          from: attributes.restart_count
          to: attributes["pod_restart_count"]
        - type: move
          from: attributes.uid
          to: attributes["pod_uid"]
        # Optionally extract log level from JSON logs
        - type: json_parser
          id: extract-level-from-json-log
          parse_from: attributes["log"]
          output: parser-level
          if: 'attributes["log"] matches "^{.*}$"'
        - type: regex_parser
          id: parser-level
          regex: "^.*level=(?P<level>.*) msg=.*$"
          parse_from: body
          if: 'body matches ".*level=.* msg.*"'
        - type: severity_parser
          parse_from: attributes["level"]
          if: 'attributes["level"] != nil'

  processors:
    k8sattributes:
      filter:
        node_from_env_var: $${env:K8S_NODE_NAME}
      pod_association:
        - sources:
            - from: resource_attribute
              name: k8s.pod.ip
        - sources:
            - from: resource_attribute
              name: k8s.pod.uid
        - sources:
            - from: connection
        - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
      extract:
        metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.namespace.name
          - k8s.node.name
          - k8s.pod.start_time
    attributes:
      actions:
        - action: insert
          key: loki.attribute.labels
          value:
            - stream
            - filename
            - pod
            - namespace
            - pod_start_time
            - pod_uid
            - log.file.path
        - action: insert
          key: loki.resource.labels
          value:
            - cloud.platform
            - cloud.availability_zone
            - cloud.account.id
            - cloud.region
            - host.id
            - host.image.id
            - host.type
            - snow.id
            - otel.collector.id
            - k8s.cluster.id
    resource:
      attributes:
        - action: insert
          key: loki.format
          value: json
        - action: upsert
          key: k8s.cluster.id
          value: ${eks_cluster}
        - action: upsert
          key: otel.collector.id
          value: ${collector_id}-logs
        - action: upsert
    resourcedetection:
      detectors: [env, system, ec2]
      timeout: 2s
      override: true
    filter/noise:
      logs:
        log_record:
          - 'IsMatch(body, ".*debug.*")'
    batch:
      send_batch_max_size: 1000
      timeout: 10s
      send_batch_size: 800

  extensions:
    k8s_observer:
      node: $${K8S_NODE_NAME}
      observe_pods: true
      observe_nodes: true
    health_check: {}
    headers_setter:
      headers:
        - action: upsert
          key: X-Scope-OrgId
          from_context: X-Scope-OrgId

  exporters:
    otlp:
      endpoint: "https://otel.varidha.com:443"
      tls:
        insecure_skip_verify: true
        insecure: true
      headers:
        X-Scope-OrgId: obsrv
      compression: snappy

  service:
    telemetry:
      metrics:
        address: 0.0.0.0:8888
        level: detailed
      logs:
        level: WARN
    extensions: [health_check, headers_setter]
    pipelines:
      logs:
        receivers: [filelog, k8s_events]
        processors:
          [
            resource,
            attributes,
            resourcedetection,
            k8sattributes,
            filter/noise,
            batch,
          ]
        exporters: [otlp]

ports:
  otlp:
    enabled: false
  otlp-http:
    enabled: false
  jaeger-compact:
    enabled: false
  jaeger-thrift:
    enabled: false
  jaeger-grpc:
    enabled: false
  zipkin:
    enabled: false
  metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    protocol: TCP

# Resource limits & requests. Uncomment and adjust if needed.
resources: {}
# resources:
#   requests:
#     cpu: 250m
#     memory: 256Mi
#   limits:
#     cpu: 500m
#     memory: 512Mi
